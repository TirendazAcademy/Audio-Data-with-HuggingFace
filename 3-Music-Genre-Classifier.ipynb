{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TirendazAcademy/Audio-Data-with-HuggingFace/blob/main/3-Music-Genre-Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcmbkLrHpxjL"
      },
      "source": [
        "# Installing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSOyqDYRgl9C"
      },
      "source": [
        "Let's install the trasformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U18FRzPifcqP"
      },
      "outputs": [],
      "source": [
        "!pip install -qU git+https://github.com/huggingface/transformers\n",
        "!pip install -qU git+https://github.com/huggingface/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fii1QHHhhDyP"
      },
      "outputs": [],
      "source": [
        "import transformers, datasets\n",
        "print(\"The transformers version:\", transformers.__version__)\n",
        "print(\"The datasets version:\", datasets.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhpbB00ihLKJ"
      },
      "source": [
        "Next, let's load the dataset we'll use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-ewZRfihIif"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFgXN0XRieKJ"
      },
      "source": [
        "#  Pre-trained models and datasets for audio classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihmBexMDh7sM"
      },
      "source": [
        "Then let's create our pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGMI7ZZdhQPX"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"audio-classification\",\n",
        "    model=\"anton-l/xtreme_s_xlsr_300m_minds14\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly86sGtkiFW8"
      },
      "source": [
        "Finally, we can pass a sample to the classification pipeline to make a prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDpRcgfyiF1Q"
      },
      "outputs": [],
      "source": [
        "classifier(minds[0][\"audio\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI2gvUSDiWBF"
      },
      "source": [
        "## Speech Commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDjDRxSYjyGJ"
      },
      "source": [
        "First, let's load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGR9t69HiXkv"
      },
      "outputs": [],
      "source": [
        "speech_commands = load_dataset(\n",
        "    \"speech_commands\", \"v0.02\", split=\"validation\", streaming=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWDQacr-j5Rn"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(speech_commands))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGM-i1Ltj_Jm"
      },
      "outputs": [],
      "source": [
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-fppD-tkN0r"
      },
      "source": [
        " Let's load an official Audio Spectrogram Transformer checkpoint fine-tuned on the Speech Commands dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEE_IfcKkPbQ"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline(\n",
        "    \"audio-classification\", model=\"MIT/ast-finetuned-speech-commands-v2\"\n",
        ")\n",
        "classifier(sample[\"audio\"].copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsRkxBYfkiJf"
      },
      "source": [
        "As you can see, the prediction of our model is backward. Let me take a listen to the sample and verify this is prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFaD2I1TkvVZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6GDtVwqmcHi"
      },
      "source": [
        "## Language Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSdiS8uBmk9G"
      },
      "source": [
        "Language identification (LID) is the task of identifying the language spoken in an audio sample from a list of candidate languages systems in 102 languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgbUTNS_k3In"
      },
      "outputs": [],
      "source": [
        "fleurs = load_dataset(\"google/fleurs\", \"all\", split=\"validation\", streaming=True, trust_remote_code=True)\n",
        "sample = next(iter(fleurs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGTmNdnznDxb"
      },
      "source": [
        "Next, let's create our pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYxYj7iqmwGK"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline(\n",
        "    \"audio-classification\", model=\"sanchit-gandhi/whisper-medium-fleurs-lang-id\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doToFKYdns9X"
      },
      "source": [
        "We can then pass the audio through our classifier and generate a prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ge7On14nqPo"
      },
      "outputs": [],
      "source": [
        "classifier(sample[\"audio\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyMumlaooAuU"
      },
      "source": [
        "## Zero-Shot Audio Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeFgPOr8oBn8"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"ashraq/esc50\", split=\"train\", streaming=True)\n",
        "audio_sample = next(iter(dataset))[\"audio\"][\"array\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK863iorokEt"
      },
      "outputs": [],
      "source": [
        "candidate_labels = [\"Sound of a dog\", \"Sound of vacuum cleaner\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDvnYBVco2gM"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline(\n",
        "    task=\"zero-shot-audio-classification\", model=\"laion/clap-htsat-unfused\"\n",
        ")\n",
        "classifier(audio_sample, candidate_labels=candidate_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdvrK-vqpI38"
      },
      "outputs": [],
      "source": [
        "Audio(audio_sample, rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning a model for music classification"
      ],
      "metadata": {
        "id": "Kh-xujqpBcJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "gtzan = load_dataset(\"marsyas/gtzan\", \"all\", trust_remote_code=True)\n",
        "gtzan"
      ],
      "metadata": {
        "id": "VvwBcviaBdHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no a test set. Let's create one."
      ],
      "metadata": {
        "id": "pp5ofp3ZBt_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan = gtzan[\"train\"].train_test_split(seed=42, shuffle=True, test_size=0.1)\n",
        "gtzan"
      ],
      "metadata": {
        "id": "uiXf42NKBp5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan[\"train\"][0]"
      ],
      "metadata": {
        "id": "VkC2kG_zByhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s use the int2str() method of the genre feature to map these integers to human-readable names:"
      ],
      "metadata": {
        "id": "Ed_C8nAQCJdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan[\"train\"].features[\"genre\"]"
      ],
      "metadata": {
        "id": "poAKUZh4CNon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label_fn = gtzan[\"train\"].features[\"genre\"].int2str\n",
        "id2label_fn(gtzan[\"train\"][0][\"genre\"])"
      ],
      "metadata": {
        "id": "Pcwcf0NACKS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s now listen to a few more examples by using Gradio"
      ],
      "metadata": {
        "id": "iYLIYTnsCsW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "qGJ-96geCfl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def generate_audio():\n",
        "    example = gtzan[\"train\"].shuffle()[0]\n",
        "    audio = example[\"audio\"]\n",
        "    return (\n",
        "        audio[\"sampling_rate\"],\n",
        "        audio[\"array\"],\n",
        "    ), id2label_fn(example[\"genre\"])\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Column():\n",
        "        for _ in range(4):\n",
        "            audio, label = generate_audio()\n",
        "            output = gr.Audio(audio, label=label)\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "k9vXYMXhCaDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From audio to machine learning features\n",
        "\n",
        "###  Preprocessing the data\n",
        "\n",
        "Letâ€™s begin by instantiating the feature extractor for DistilHuBERT from the pre-trained checkpoint:"
      ],
      "metadata": {
        "id": "mNzDsisWE42H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "model_id = \"ntu-spml/distilhubert\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    model_id, do_normalize=True, return_attention_mask=True\n",
        ")"
      ],
      "metadata": {
        "id": "9BSQuwZSC_qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weâ€™ll have to resample the audio file to 16,000 Hz before passing it to the feature extractor."
      ],
      "metadata": {
        "id": "OEDH-4UoFd1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_rate = feature_extractor.sampling_rate\n",
        "sampling_rate"
      ],
      "metadata": {
        "id": "1Qd91KgJFWWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "gtzan = gtzan.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
      ],
      "metadata": {
        "id": "aHEHC3wpFiRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now check the first sample of the train-split of our dataset to verify that it is indeed at 16,000 Hz."
      ],
      "metadata": {
        "id": "j0rNddbJFqEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan[\"train\"][0]"
      ],
      "metadata": {
        "id": "18dDFEO6FmyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the feature extractor in operation by applying it to our first audio sample. First, letâ€™s compute the mean and variance of our raw audio data:"
      ],
      "metadata": {
        "id": "kvfj_eLMGThd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sample = gtzan[\"train\"][0][\"audio\"]\n",
        "\n",
        "print(f\"Mean: {np.mean(sample['array']):.3}, Variance: {np.var(sample['array']):.3}\")"
      ],
      "metadata": {
        "id": "_7xnXG5SFsUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s apply the feature extractor and see what the outputs look like:"
      ],
      "metadata": {
        "id": "XJqe65EoGhLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
        "\n",
        "print(f\"inputs keys: {list(inputs.keys())}\")\n",
        "\n",
        "print(\n",
        "    f\"Mean: {np.mean(inputs['input_values']):.3}, Variance: {np.var(inputs['input_values']):.3}\"\n",
        ")"
      ],
      "metadata": {
        "id": "e31ABRcbGhpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's truncate the audio clips to be 30 seconds in length."
      ],
      "metadata": {
        "id": "SpBa3r_nHLkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_duration = 30.0\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
        "    inputs = feature_extractor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=feature_extractor.sampling_rate,\n",
        "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "luIVt_0eHR7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We can now apply it to the dataset using the map() method."
      ],
      "metadata": {
        "id": "a0Re3BdiHaax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan_encoded = gtzan.map(\n",
        "    preprocess_function,\n",
        "    remove_columns=[\"audio\", \"file\"],\n",
        "    batched=True,\n",
        "    batch_size=100,\n",
        "    num_proc=1,\n",
        ")\n",
        "gtzan_encoded"
      ],
      "metadata": {
        "id": "Gb82ifb_HZuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan_encoded = gtzan_encoded.rename_column(\"genre\", \"label\")"
      ],
      "metadata": {
        "id": "DS8yk_QUH1as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to obtain the label mappings from the dataset."
      ],
      "metadata": {
        "id": "4ut_Ud0XH-XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    str(i): id2label_fn(i)\n",
        "    for i in range(len(gtzan_encoded[\"train\"].features[\"label\"].names))\n",
        "}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "id2label[\"7\"]"
      ],
      "metadata": {
        "id": "_A3gyqH2H2ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, weâ€™ve now got a dataset thatâ€™s ready for training! Letâ€™s take a look at how we can train a model on this dataset."
      ],
      "metadata": {
        "id": "bFTLkU8-IFnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning the model"
      ],
      "metadata": {
        "id": "40btbnXaIICb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fine-tune the model, weâ€™ll use the Trainer class from ðŸ¤— Transformers"
      ],
      "metadata": {
        "id": "MKhItLJIIUK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForAudioClassification\n",
        "\n",
        "num_labels = len(id2label)\n",
        "\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    model_id,\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        ")"
      ],
      "metadata": {
        "id": "mS6R1xIbIBOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "CtRYVNg-Ifw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id"
      ],
      "metadata": {
        "id": "WOjdB2opJDNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id.split(\"/\")[-1]"
      ],
      "metadata": {
        "id": "o6H7SzpOJBIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "model_name = model_id.split(\"/\")[-1]\n",
        "batch_size = 8\n",
        "gradient_accumulation_steps = 1\n",
        "num_train_epochs = 10\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-gtzan\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sciF-j7EI49G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the metrics."
      ],
      "metadata": {
        "id": "YtqPFvw5KOWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate"
      ],
      "metadata": {
        "id": "WW6F05URKUxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
      ],
      "metadata": {
        "id": "XQDloZVyKHCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s instantiate the Trainer and train the model:"
      ],
      "metadata": {
        "id": "vSXfnHzBKfey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=gtzan_encoded[\"train\"],\n",
        "    eval_dataset=gtzan_encoded[\"test\"],\n",
        "    tokenizer=feature_extractor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "JfO76gs_KbgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading the hub"
      ],
      "metadata": {
        "id": "8wbUe35CL4LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"marsyas/gtzan\",\n",
        "    \"dataset\": \"GTZAN\",\n",
        "    \"model_name\": f\"{model_name}-finetuned-gtzan-evren\",\n",
        "    \"finetuned_from\": model_id,\n",
        "    \"tasks\": \"audio-classification\",\n",
        "}"
      ],
      "metadata": {
        "id": "--zn0YoJLxVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "id": "g5ssVx9oL7RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sharing the model"
      ],
      "metadata": {
        "id": "A3GWfJIHMYhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"audio-classification\", model=\"sanchit-gandhi/distilhubert-finetuned-gtzan\"\n",
        ")"
      ],
      "metadata": {
        "id": "LINvFuMWMasz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNJCWYFWy8nVJ7DjGhVK6Oe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}