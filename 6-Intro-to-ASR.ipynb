{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPDBuWKKNJiYANI1vOPi6lJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TirendazAcademy/Audio-Data-with-HuggingFace/blob/main/6-Intro-to-ASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "YChZ_JniOW0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working-with-Audio-Data"
      ],
      "metadata": {
        "id": "-ydgwSpoOQkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpWWH2cONtQc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset_builder\n",
        "\n",
        "ds_builder = load_dataset_builder(\"openslr/librispeech_asr\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_builder.info.splits"
      ],
      "metadata": {
        "id": "aJmyp7GyO1jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_builder.info.features"
      ],
      "metadata": {
        "id": "MB-JocLNO-_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\n",
        "    \"openslr/librispeech_asr\",\n",
        "    split=\"train.clean.360\",\n",
        "    streaming=True, trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "d4aB1AKoPblH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(ds))\n",
        "sample"
      ],
      "metadata": {
        "id": "rRhr_6tkQiB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array = sample[\"audio\"][\"array\"]\n",
        "array"
      ],
      "metadata": {
        "id": "ZwcoA-W8SSC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_rate = sample[\"audio\"][\"sampling_rate\"]\n",
        "sampling_rate"
      ],
      "metadata": {
        "id": "db9AAEb2SWmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get the first 5 seconds\n",
        "array = array[: sampling_rate * 5]\n",
        "print(f\"Number of samples: {len(array)}. Values: {array}\")"
      ],
      "metadata": {
        "id": "_0DJO1QCSUkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio(data=array, rate=sampling_rate)"
      ],
      "metadata": {
        "id": "mdqURZb5SxeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa.display\n",
        "librosa.display.waveshow(array, sr=sampling_rate);"
      ],
      "metadata": {
        "id": "BwVxrojtTwJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_sine(freq):\n",
        "    sr = 1000  # samples per second\n",
        "    ts = 1.0 / sr  # sampling interval\n",
        "    t = np.arange(0, 1, ts)  # time vector\n",
        "    amplitude = np.sin(2 * np.pi * freq * t)\n",
        "\n",
        "    plt.plot(t, amplitude)\n",
        "    plt.title(\"Sine wave with frequency {}\".format(freq))\n",
        "    plt.xlabel(\"Time\")\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plot_sine(1)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plot_sine(2)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plot_sine(5)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plot_sine(30)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jUbwBf2ZbHcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-tiny\",\n",
        "    max_new_tokens=100,\n",
        ")\n",
        "pipe(array)"
      ],
      "metadata": {
        "id": "HoLX1yaEg1N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample[\"text\"]"
      ],
      "metadata": {
        "id": "PqtcmxCFuRUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder-Based Techniques"
      ],
      "metadata": {
        "id": "DNoLixOR46Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ZanHVo1Fykeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "# The Wav2Vec2Processor has the pre- and post-processing incorporated\n",
        "wav2vec2_processor = Wav2Vec2Processor.from_pretrained(\n",
        "    \"facebook/wav2vec2-base-960h\"\n",
        ")\n",
        "wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-base-960h\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "YTokM3_Zx6Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run forward pass, making sure to resample to 16kHz\n",
        "inputs = wav2vec2_processor(\n",
        "    array, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "tVYrR3oAuT9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    outputs = wav2vec2_model(**inputs.to(device))"
      ],
      "metadata": {
        "id": "dE04S8i_y6B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe\n",
        "predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "transcription = wav2vec2_processor.batch_decode(predicted_ids)\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "zOvnOmaky__2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder-Decoder Techniques"
      ],
      "metadata": {
        "id": "Joc5sdyI40lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\n",
        "    \"openai/whisper-small\", language=\"Spanish\", task=\"transcribe\"\n",
        ")"
      ],
      "metadata": {
        "id": "T8XUGcfO4wYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"Hola, ¿cómo estás?\"\n",
        "labels = tokenizer(input_str).input_ids\n",
        "labels"
      ],
      "metadata": {
        "id": "krW3Gw764yhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_with_special = tokenizer.decode(\n",
        "    labels, skip_special_tokens=False\n",
        ")\n",
        "decoded_with_special"
      ],
      "metadata": {
        "id": "Skp1hlZj5bhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
        "decoded_str"
      ],
      "metadata": {
        "id": "yFmmv-Gu5gq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input:                         {input_str}\")\n",
        "print(f\"Formatted input w/ special:    {decoded_with_special}\")\n",
        "print(f\"Formatted input w/out special: {decoded_str}\")"
      ],
      "metadata": {
        "id": "JMgFrhJD5pU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
        "\n",
        "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "whisper_model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    \"openai/whisper-small\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "oSrZmLRw6xDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = whisper_processor(\n",
        "    array, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "1C3ZXwkT60i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "id": "_2FL1Aqt854y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    generated_ids = whisper_model.generate(**inputs.to(device))"
      ],
      "metadata": {
        "id": "6dvGluDL826B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = whisper_processor.batch_decode(\n",
        "    generated_ids, skip_special_tokens=False\n",
        ")[0]\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "ZjehpGPj8-Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Model to Pipeline"
      ],
      "metadata": {
        "id": "GUXrrqtH9WGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install genaibook"
      ],
      "metadata": {
        "collapsed": true,
        "id": "M6jtNwK_9cSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from genaibook.core import generate_long_audio"
      ],
      "metadata": {
        "id": "AchhYwA3_bCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from genaibook.core import generate_long_audio\n",
        "\n",
        "long_audio = generate_long_audio()"
      ],
      "metadata": {
        "id": "WORH7jEl9NmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_audio"
      ],
      "metadata": {
        "id": "W7GcfhcI_nex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio(data=long_audio, rate=16000)"
      ],
      "metadata": {
        "id": "-mB5rY6o_sqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\", model=\"openai/whisper-small\", device=device\n",
        ")"
      ],
      "metadata": {
        "id": "WwdCSYt19Ztp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\n",
        "    long_audio,\n",
        "    generate_kwargs={\"task\": \"transcribe\"},\n",
        "    chunk_length_s=5,\n",
        "    batch_size=8,\n",
        "    return_timestamps=True,\n",
        ")"
      ],
      "metadata": {
        "id": "XPk54n8JAUUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "QOhyDLy6BAyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from genaibook.core import measure_latency_and_memory_use\n",
        "\n",
        "wav2vec2_pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"facebook/wav2vec2-base-960h\",\n",
        "    device=device,\n",
        ")\n",
        "whisper_pipe = pipeline(\n",
        "    \"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device\n",
        ")\n",
        "\n",
        "with torch.inference_mode():\n",
        "    measure_latency_and_memory_use(\n",
        "        wav2vec2_pipe, array, \"Wav2Vec2\", device, nb_loops=100\n",
        "    )\n",
        "    measure_latency_and_memory_use(\n",
        "        whisper_pipe, array, \"Whisper\", device=device, nb_loops=100\n",
        "    )"
      ],
      "metadata": {
        "id": "IbeKvHfuA_Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Error Rate (WER)"
      ],
      "metadata": {
        "id": "KLw6SfPCB4iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "wer_metric = load(\"wer\")\n",
        "\n",
        "label = \"how can the llama jump\"\n",
        "pred = \"can the lama jump up\"\n",
        "wer = wer_metric.compute(references=[label], predictions=[pred])\n",
        "\n",
        "print(wer)"
      ],
      "metadata": {
        "id": "ZmCzLQjpBjPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Normalizing"
      ],
      "metadata": {
        "id": "h_YahDkJCAPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "normalizer = BasicTextNormalizer()\n",
        "print(normalizer(\"I'm having a great day!\"))"
      ],
      "metadata": {
        "id": "5f0TJMltB0KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "def normalize(batch):\n",
        "    batch[\"norm_text\"] = normalizer(batch[\"sentence\"])\n",
        "    return batch\n",
        "\n",
        "def prepare_dataset(language=\"en\", sample_count=200):\n",
        "    dataset = load_dataset(\n",
        "        \"mozilla-foundation/common_voice_13_0\",\n",
        "        language,\n",
        "        split=\"test\",\n",
        "        streaming=True,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "    dataset = dataset.take(sample_count)\n",
        "    buffered_dataset = [sample for sample in dataset.map(normalize)]\n",
        "    return buffered_dataset\n",
        "\n",
        "def evaluate_model(pipe, dataset, lang=\"en\", use_whisper=False):\n",
        "    predictions, references = [], []\n",
        "\n",
        "    for sample in dataset:\n",
        "        if use_whisper:\n",
        "            extra_kwargs = {\n",
        "                \"task\": \"transcribe\",\n",
        "                \"language\": f\"<|{lang}|>\",\n",
        "                \"max_new_tokens\": 100,\n",
        "            }\n",
        "            transcription = pipe(\n",
        "                sample[\"audio\"][\"array\"],\n",
        "                return_timestamps=True,\n",
        "                generate_kwargs=extra_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            transcription = pipe(sample[\"audio\"][\"array\"])\n",
        "        predictions.append(normalizer(transcription[\"text\"]))\n",
        "        references.append(sample[\"norm_text\"])\n",
        "    return predictions, references"
      ],
      "metadata": {
        "id": "uTxKlKSPCPp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_suite = [\n",
        "    [\"Wav2Vec2\", wav2vec2_pipe, \"en\"],\n",
        "    [\"Wav2Vec2\", wav2vec2_pipe, \"fr\"],\n",
        "    [\"Whisper\", whisper_pipe, \"en\"],\n",
        "    [\"Whisper\", whisper_pipe, \"fr\"],\n",
        "]"
      ],
      "metadata": {
        "id": "kkrxtLNMCkod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cer_metric = load(\"cer\")"
      ],
      "metadata": {
        "id": "cM6HLRo1DInS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process the English and French datasets\n",
        "processed_datasets = {\n",
        "    \"en\": prepare_dataset(\"en\"),\n",
        "    \"fr\": prepare_dataset(\"fr\"),\n",
        "}"
      ],
      "metadata": {
        "id": "QbI2m6rMDPWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for config in eval_suite:\n",
        "    model_name, pipeline, lang = config[0], config[1], config[2]\n",
        "\n",
        "    dataset = processed_datasets[lang]\n",
        "\n",
        "    predictions, references = evaluate_model(\n",
        "        pipeline, dataset, lang, model_name == \"Whisper\"\n",
        "    )\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    wer = wer_metric.compute(references=references, predictions=predictions)\n",
        "    cer = cer_metric.compute(references=references, predictions=predictions)\n",
        "\n",
        "    print(f\"{model_name} metrics for lang: {lang}. WER: {wer}, CER: {cer}\")"
      ],
      "metadata": {
        "id": "5tFZg8gqDVii"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}