{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMb/95vV9a58pGCxceWtf1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TirendazAcademy/Audio-Data-with-HuggingFace/blob/main/4-Automatic-Speech-Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers.__version__"
      ],
      "metadata": {
        "id": "iPhEOvuoNqi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bMXxI8yDNyKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained models for automatic speech recognition"
      ],
      "metadata": {
        "id": "9UqKHOwyOBey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iQvBfJXNcVx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"audio\"][2]"
      ],
      "metadata": {
        "id": "i45fQX41OLDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "sample = dataset[2]\n",
        "\n",
        "print(sample[\"text\"])\n",
        "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "SSSyw_3JOJPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-100h\")"
      ],
      "metadata": {
        "id": "pGvTy_-DO_Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(sample[\"audio\"].copy())"
      ],
      "metadata": {
        "id": "HAGUoQY6PPMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graduation to Seq2Seq"
      ],
      "metadata": {
        "id": "5D2jLKC8QsTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device\n",
        ")"
      ],
      "metadata": {
        "id": "B_a6CeBLQtWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(sample[\"audio\"], max_new_tokens=256)"
      ],
      "metadata": {
        "id": "tL_TJxUuTinY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"facebook/multilingual_librispeech\", \"spanish\", split=\"test\", streaming=True\n",
        ")\n",
        "sample = next(iter(dataset))"
      ],
      "metadata": {
        "id": "uR3iO__vT0jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "id": "Kn74eE2UVGG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample[\"transcript\"])\n",
        "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "AvUBWFc8UTNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\": \"transcribe\"})"
      ],
      "metadata": {
        "id": "PVXFamWEWCGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(sample[\"audio\"], max_new_tokens=256, generate_kwargs={\"task\": \"translate\"})"
      ],
      "metadata": {
        "id": "zCAIVWvTWcJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long-Form Transcription and Timestamps"
      ],
      "metadata": {
        "id": "Wza_ollAWkuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "target_length_in_m = 5\n",
        "\n",
        "# convert from minutes to seconds (* 60) to num samples (* sampling rate)\n",
        "sampling_rate = pipe.feature_extractor.sampling_rate\n",
        "target_length_in_samples = target_length_in_m * 60 * sampling_rate\n",
        "\n",
        "# iterate over our streaming dataset, concatenating samples until we hit our target\n",
        "long_audio = []\n",
        "for sample in dataset:\n",
        "    long_audio.extend(sample[\"audio\"][\"array\"])\n",
        "    if len(long_audio) > target_length_in_samples:\n",
        "        break\n",
        "\n",
        "long_audio = np.asarray(long_audio)\n",
        "\n",
        "# how did we do?\n",
        "seconds = len(long_audio) / 16000\n",
        "minutes, seconds = divmod(seconds, 60)\n",
        "print(f\"Length of audio sample is {minutes} minutes {seconds:.2f} seconds\")"
      ],
      "metadata": {
        "id": "Vuwbu1fhWkMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\n",
        "    long_audio,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"task\": \"transcribe\"},\n",
        "    chunk_length_s=30,\n",
        "    batch_size=8,\n",
        ")"
      ],
      "metadata": {
        "id": "ZAfSEP4KXV2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\n",
        "    long_audio,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"task\": \"transcribe\"},\n",
        "    chunk_length_s=30,\n",
        "    batch_size=8,\n",
        "    return_timestamps=True,\n",
        ")[\"chunks\"]"
      ],
      "metadata": {
        "id": "MaONauMhXgmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metrics for ASR"
      ],
      "metadata": {
        "id": "UWQtsSR-X2Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU evaluate jiwer"
      ],
      "metadata": {
        "id": "06GGRSNSX7ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = \"the cat sat on the mat\"\n",
        "prediction = \"the cat sit on the\""
      ],
      "metadata": {
        "id": "YxNUVfVUn3Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "wer_metric = load(\"wer\")\n",
        "\n",
        "wer = wer_metric.compute(references=[reference], predictions=[prediction])\n",
        "\n",
        "print(wer)"
      ],
      "metadata": {
        "id": "pJgsDL10nzz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "prediction = \" He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.\"\n",
        "normalized_prediction = normalizer(prediction)\n",
        "\n",
        "normalized_prediction"
      ],
      "metadata": {
        "id": "44qWoSE7qWil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = \"HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\"\n",
        "normalized_referece = normalizer(reference)\n",
        "\n",
        "wer = wer_metric.compute(\n",
        "    references=[normalized_referece], predictions=[normalized_prediction]\n",
        ")\n",
        "wer"
      ],
      "metadata": {
        "id": "HRB1Cw4gqdqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "    torch_dtype = torch.float16\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    torch_dtype = torch.float32\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-small\",\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "id": "_UfZhWpcqtfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "ZUTx6nuCq8tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "common_voice_test = load_dataset(\n",
        "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\", trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "0NAMXU_lrI0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "all_predictions = []\n",
        "\n",
        "# run streamed inference\n",
        "for prediction in tqdm(\n",
        "    pipe(\n",
        "        KeyDataset(common_voice_test, \"audio\"),\n",
        "        max_new_tokens=128,\n",
        "        generate_kwargs={\"task\": \"transcribe\"},\n",
        "        batch_size=32,\n",
        "    ),\n",
        "    total=len(common_voice_test),\n",
        "):\n",
        "    all_predictions.append(prediction[\"text\"])"
      ],
      "metadata": {
        "id": "znlXy7tMrjWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "wer_metric = load(\"wer\")\n",
        "\n",
        "wer_ortho = 100 * wer_metric.compute(\n",
        "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
        ")\n",
        "wer_ortho"
      ],
      "metadata": {
        "id": "QXc0KVZvrw7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "# compute normalised WER\n",
        "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
        "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]\n",
        "\n",
        "# filtering step to only evaluate the samples that correspond to non-zero references\n",
        "all_predictions_norm = [\n",
        "    all_predictions_norm[i]\n",
        "    for i in range(len(all_predictions_norm))\n",
        "    if len(all_references_norm[i]) > 0\n",
        "]\n",
        "all_references_norm = [\n",
        "    all_references_norm[i]\n",
        "    for i in range(len(all_references_norm))\n",
        "    if len(all_references_norm[i]) > 0\n",
        "]\n",
        "\n",
        "wer = 100 * wer_metric.compute(\n",
        "    references=all_references_norm, predictions=all_predictions_norm\n",
        ")\n",
        "\n",
        "wer"
      ],
      "metadata": {
        "id": "ROF29dE_r3Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning the ASR model"
      ],
      "metadata": {
        "id": "EbF7qGgysgJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Environment"
      ],
      "metadata": {
        "id": "_G9V5ERuuMa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\n",
        "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\"\n",
        ")\n",
        "common_voice[\"test\"] = load_dataset(\n",
        "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "ql__3wV8shOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])"
      ],
      "metadata": {
        "id": "tQplLSTVvd7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice"
      ],
      "metadata": {
        "id": "TX7IadO3vvv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
        "\n",
        "TO_LANGUAGE_CODE"
      ],
      "metadata": {
        "id": "aUIeZZLjv78c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    \"openai/whisper-small\", language=\"sinhalese\", task=\"transcribe\"\n",
        ")"
      ],
      "metadata": {
        "id": "Esg4ehRuwm0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Process the Data"
      ],
      "metadata": {
        "id": "9i4MfW4FyKhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"].features"
      ],
      "metadata": {
        "id": "G3UdMVzqxGcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "sampling_rate = processor.feature_extractor.sampling_rate\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
      ],
      "metadata": {
        "id": "NB_n0V9pyQIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice"
      ],
      "metadata": {
        "id": "GGoKRpg7zH4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"]"
      ],
      "metadata": {
        "id": "aaYWM7wIywhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"].features[\"audio\"][1]"
      ],
      "metadata": {
        "id": "NYTqEsm2zJRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"].features[\"sentence\"]"
      ],
      "metadata": {
        "id": "eEaInDH8zaZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    example = processor(\n",
        "        audio=audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        text=example[\"sentence\"],\n",
        "    )\n",
        "\n",
        "    # compute input length of audio sample in seconds\n",
        "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "pTbJtd8ryhI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.map(\n",
        "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
        ")"
      ],
      "metadata": {
        "id": "kUxnb17kzlPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice"
      ],
      "metadata": {
        "id": "t3ep87TZ0Ekz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we filter any training data with audio samples longer than 30s."
      ],
      "metadata": {
        "id": "0y9s7C1Dz7EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 30.0\n",
        "\n",
        "def is_audio_in_length_range(length):\n",
        "    return length < max_input_length"
      ],
      "metadata": {
        "id": "cYvaCQtOzs1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
        "    is_audio_in_length_range,\n",
        "    input_columns=[\"input_length\"],\n",
        ")"
      ],
      "metadata": {
        "id": "M9v8YfiG0AFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"]"
      ],
      "metadata": {
        "id": "Lvv79jbg0afK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "9JTBjg7t0qBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Data Collator"
      ],
      "metadata": {
        "id": "IfG6e4Yu01W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [\n",
        "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
        "        ]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "Yn9utbNJ0q4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "JIRuGAfW1pH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics"
      ],
      "metadata": {
        "id": "CM1z-qZe1ri7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "i7ZLGmjp1sax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # compute orthographic wer\n",
        "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    # compute normalised WER\n",
        "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
        "    label_str_norm = [normalizer(label) for label in label_str]\n",
        "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
        "    pred_str_norm = [\n",
        "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "    label_str_norm = [\n",
        "        label_str_norm[i]\n",
        "        for i in range(len(label_str_norm))\n",
        "        if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
        "\n",
        "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}"
      ],
      "metadata": {
        "id": "S6zJXoAz1vgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load a Pre-Trained Checkpoint"
      ],
      "metadata": {
        "id": "QiicUEZr1_nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ],
      "metadata": {
        "id": "3ewPjxyf16ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "# disable cache during training since it's incompatible with gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "\n",
        "# set language and task for generation and re-enable cache\n",
        "model.generate = partial(\n",
        "    model.generate, language=\"sinhalese\", task=\"transcribe\", use_cache=True\n",
        ")"
      ],
      "metadata": {
        "id": "t2zcy3zT2Chq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Training Configuration"
      ],
      "metadata": {
        "id": "1hwhHdy52O10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-dv\",  # name on the HF Hub\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_steps=50,\n",
        "    max_steps=500,  # increase to 4000 if you have your own GPU or a Colab paid plan\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    fp16_full_eval=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "QR7YghDP2MK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor,\n",
        ")"
      ],
      "metadata": {
        "id": "ICyo6i9Z2UBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "yW-DZnOX2bsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
        "    \"dataset\": \"Common Voice 13\",  # a 'pretty' name for the training dataset\n",
        "    \"language\": \"dv\",\n",
        "    \"model_name\": \"Whisper Small Dv - Sanchit Gandhi\",  # a 'pretty' name for your model\n",
        "    \"finetuned_from\": \"openai/whisper-small\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "}"
      ],
      "metadata": {
        "id": "rxkjv5s62juD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "id": "RvYX_B4K234B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sharing Your Model"
      ],
      "metadata": {
        "id": "2vg2Im_G28mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"sanchit-gandhi/whisper-small-dv\")"
      ],
      "metadata": {
        "id": "lgLPnnLG29or"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}